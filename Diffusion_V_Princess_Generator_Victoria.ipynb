{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1YwMUyt9LHG1"
   },
   "source": [
    "# Princess Generator: Ver. Victoria\n",
    "## Attention: Not colab ready. You have to install dependencies and download the models, put them in right position by yourself.\n",
    "### It would be great if someone could get this to a colab ready state... \n",
    "\n",
    "Originally by Katherine Crowson (https://github.com/crowsonkb, https://twitter.com/RiversHaveWings). It uses a 512x512 unconditional ImageNet diffusion model fine-tuned from OpenAI's 512x512 class-conditional ImageNet diffusion model (https://github.com/openai/guided-diffusion) together with CLIP (https://github.com/openai/CLIP) to connect text prompts with images. \n",
    "\n",
    "Now updated using V-diffusion by Katherine Crowson. (https://github.com/crowsonkb/v-diffusion-pytorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ivuJjs6p9ttA"
   },
   "source": [
    "@Nshepperd, @DaneilRussruss also have contribution in this code.\n",
    "I may missed some of the contributors of specific functions. If you found your credit is missing, let me know and I 'll add it as soon as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "JmbrcrhpBPC6"
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from functools import partial\n",
    "import gc\n",
    "import io\n",
    "import math\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "from piq import brisque\n",
    "from itertools import product\n",
    "from IPython import display\n",
    "import lpips\n",
    "from PIL import Image, ImageOps\n",
    "import requests\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms\n",
    "from torchvision import transforms as T\n",
    "from torchvision.transforms import functional as TF\n",
    "from tqdm.notebook import tqdm\n",
    "from numpy import nan\n",
    "\n",
    "sys.path.append('./CLIP')\n",
    "sys.path.append('./v-diffusion-pytorch')\n",
    "sys.path.append('./ResizeRight/')\n",
    "\n",
    "from resize_right import resize, calc_pad_sz\n",
    "\n",
    "\n",
    "import clip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "YHOj78Yvx8jP"
   },
   "outputs": [],
   "source": [
    "# Define necessary functions\n",
    "class ReplaceGrad(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x_forward, x_backward):\n",
    "        ctx.shape = x_backward.shape\n",
    "        return x_forward\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_in):\n",
    "        return None, grad_in.sum_to_size(ctx.shape)\n",
    "\n",
    "\n",
    "replace_grad = ReplaceGrad.apply\n",
    "\n",
    "        \n",
    "def fetch(url_or_path):\n",
    "    if str(url_or_path).startswith('http://') or str(url_or_path).startswith('https://'):\n",
    "        r = requests.get(url_or_path)\n",
    "        r.raise_for_status()\n",
    "        fd = io.BytesIO()\n",
    "        fd.write(r.content)\n",
    "        fd.seek(0)\n",
    "        return fd\n",
    "    return open(url_or_path, 'rb')\n",
    "\n",
    "\n",
    "def parse_prompt(prompt):\n",
    "    if prompt.startswith('http://') or prompt.startswith('https://') or prompt.startswith(\"E:\") or prompt.startswith(\"C:\") or prompt.startswith(\"D:\"):\n",
    "        vals = prompt.rsplit(':', 2)\n",
    "        vals = [vals[0] + ':' + vals[1], *vals[2:]]\n",
    "    else:\n",
    "        vals = prompt.rsplit(':', 1)\n",
    "    vals = vals + ['', '1'][len(vals):]\n",
    "    return vals[0], float(vals[1])\n",
    "\n",
    "\n",
    "class MakeCutoutsVDango(nn.Module):\n",
    "    def __init__(self, cut_size,\n",
    "                 Overview=4, \n",
    "                 WholeCrop = 0, WC_Allowance = 10, WC_Grey_P=0.2,\n",
    "                 InnerCrop = 0, IC_Size_Pow=0.5, IC_Grey_P = 0.2\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.cut_size = cut_size\n",
    "        self.Overview = Overview\n",
    "        self.WholeCrop= WholeCrop\n",
    "        self.WC_Allowance = WC_Allowance\n",
    "        self.WC_Grey_P = WC_Grey_P\n",
    "        self.InnerCrop = InnerCrop\n",
    "        self.IC_Size_Pow = IC_Size_Pow\n",
    "        self.IC_Grey_P = IC_Grey_P\n",
    "        self.augs = T.Compose([\n",
    "            T.RandomHorizontalFlip(p=0.5),\n",
    "            T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
    "            T.RandomAffine(degrees=5, \n",
    "                           translate=(0.05, 0.05), \n",
    "                           #scale=(0.9,0.95),\n",
    "                           fill=-1,  interpolation = T.InterpolationMode.BILINEAR, ),\n",
    "            T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
    "            #T.RandomPerspective(p=1, interpolation = T.InterpolationMode.BILINEAR, fill=-1,distortion_scale=0.2),\n",
    "            T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
    "            T.RandomGrayscale(p=0.1),\n",
    "            T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
    "            T.ColorJitter(brightness=0.05, contrast=0.05, saturation=0.05),\n",
    "        ])\n",
    "\n",
    "    def forward(self, input):\n",
    "        gray = transforms.Grayscale(3)\n",
    "        sideY, sideX = input.shape[2:4]\n",
    "        max_size = min(sideX, sideY)\n",
    "        min_size = min(sideX, sideY, self.cut_size)\n",
    "        l_size = max(sideX, sideY)\n",
    "        output_shape = [1,3,self.cut_size,self.cut_size] \n",
    "        output_shape_2 = [1,3,self.cut_size+2,self.cut_size+2]\n",
    "        pad_input = F.pad(input,((sideY-max_size)//2+round(max_size*0.05),(sideY-max_size)//2+round(max_size*0.05),(sideX-max_size)//2+round(max_size*0.05),(sideX-max_size)//2+round(max_size*0.05)), **padargs)\n",
    "        cutouts_list = []\n",
    "        \n",
    "        if self.Overview>0:\n",
    "            cutouts = []\n",
    "            cutout = resize(pad_input, out_shape=output_shape)\n",
    "            if self.Overview in [1,2,4]:\n",
    "                if self.Overview>=2:\n",
    "                    cutout=torch.cat((cutout,gray(cutout)))\n",
    "                if self.Overview==4:\n",
    "                    cutout = torch.cat((cutout, TF.hflip(cutout)))\n",
    "            else:\n",
    "                output_shape_all = list(output_shape)\n",
    "                output_shape_all[0]=self.Overview\n",
    "                cutout = resize(pad_input, out_shape=output_shape_all)\n",
    "                if aug: cutout=self.augs(cutout)\n",
    "            cutouts_list.append(cutout)\n",
    "            \n",
    "        if self.InnerCrop >0:\n",
    "            cutouts=[]\n",
    "            for i in range(self.InnerCrop):\n",
    "                size = int(torch.rand([])**self.IC_Size_Pow * (max_size - min_size) + min_size)\n",
    "                offsetx = torch.randint(0, sideX - size + 1, ())\n",
    "                offsety = torch.randint(0, sideY - size + 1, ())\n",
    "                cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
    "                if i <= int(self.IC_Grey_P * self.InnerCrop):\n",
    "                    cutout = gray(cutout)\n",
    "                cutout = resize(cutout, out_shape=output_shape)\n",
    "                cutouts.append(cutout)\n",
    "            if cutout_debug:\n",
    "                TF.to_pil_image(cutouts[-1].add(1).div(2).clamp(0, 1).squeeze(0)).save(\"content/diff/cutouts/cutout_InnerCrop.jpg\",quality=99)\n",
    "            cutouts_tensor = torch.cat(cutouts)\n",
    "            cutouts=[]\n",
    "            cutouts_list.append(cutouts_tensor)\n",
    "        cutouts=torch.cat(cutouts_list)\n",
    "        return cutouts\n",
    "\n",
    "\n",
    "def spherical_dist_loss(x, y):\n",
    "    x = F.normalize(x, dim=-1)\n",
    "    y = F.normalize(y, dim=-1)\n",
    "    return (x - y).norm(dim=-1).div(2).arcsin().pow(2).mul(2)\n",
    "\n",
    "\n",
    "def tv_loss(input):\n",
    "    \"\"\"L2 total variation loss, as in Mahendran et al.\"\"\"\n",
    "    input = F.pad(input, (0, 1, 0, 1), 'replicate')\n",
    "    x_diff = input[..., :-1, 1:] - input[..., :-1, :-1]\n",
    "    y_diff = input[..., 1:, :-1] - input[..., :-1, :-1]\n",
    "    return (x_diff**2 + y_diff**2).mean([1, 2, 3])\n",
    "\n",
    "\n",
    "def range_loss(input, range_min, range_max):\n",
    "    return (input - input.clamp(range_min,range_max)).pow(2).mean([1, 2, 3])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the secondary diffusion model\n",
    "\n",
    "def append_dims(x, n):\n",
    "    return x[(Ellipsis, *(None,) * (n - x.ndim))]\n",
    "\n",
    "\n",
    "def expand_to_planes(x, shape):\n",
    "    return append_dims(x, len(shape)).repeat([1, 1, *shape[2:]])\n",
    "\n",
    "\n",
    "def alpha_sigma_to_t(alpha, sigma):\n",
    "    return torch.atan2(sigma, alpha) * 2 / math.pi\n",
    "\n",
    "\n",
    "def t_to_alpha_sigma(t):\n",
    "    return torch.cos(t * math.pi / 2), torch.sin(t * math.pi / 2)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DiffusionOutput:\n",
    "    v: torch.Tensor\n",
    "    pred: torch.Tensor\n",
    "    eps: torch.Tensor\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Sequential):\n",
    "    def __init__(self, c_in, c_out):\n",
    "        super().__init__(\n",
    "            nn.Conv2d(c_in, c_out, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "\n",
    "class SkipBlock(nn.Module):\n",
    "    def __init__(self, main, skip=None):\n",
    "        super().__init__()\n",
    "        self.main = nn.Sequential(*main)\n",
    "        self.skip = skip if skip else nn.Identity()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return torch.cat([self.main(input), self.skip(input)], dim=1)\n",
    "\n",
    "\n",
    "class FourierFeatures(nn.Module):\n",
    "    def __init__(self, in_features, out_features, std=1.):\n",
    "        super().__init__()\n",
    "        assert out_features % 2 == 0\n",
    "        self.weight = nn.Parameter(torch.randn([out_features // 2, in_features]) * std)\n",
    "\n",
    "    def forward(self, input):\n",
    "        f = 2 * math.pi * input @ self.weight.T\n",
    "        return torch.cat([f.cos(), f.sin()], dim=-1)\n",
    "\n",
    "class SecondaryDiffusionImageNet2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        c = 64  # The base channel count\n",
    "        cs = [c, c * 2, c * 2, c * 4, c * 4, c * 8]\n",
    "\n",
    "        self.timestep_embed = FourierFeatures(1, 16)\n",
    "        self.down = nn.AvgPool2d(2)\n",
    "        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            ConvBlock(3 + 16, cs[0]),\n",
    "            ConvBlock(cs[0], cs[0]),\n",
    "            SkipBlock([\n",
    "                self.down,\n",
    "                ConvBlock(cs[0], cs[1]),\n",
    "                ConvBlock(cs[1], cs[1]),\n",
    "                SkipBlock([\n",
    "                    self.down,\n",
    "                    ConvBlock(cs[1], cs[2]),\n",
    "                    ConvBlock(cs[2], cs[2]),\n",
    "                    SkipBlock([\n",
    "                        self.down,\n",
    "                        ConvBlock(cs[2], cs[3]),\n",
    "                        ConvBlock(cs[3], cs[3]),\n",
    "                        SkipBlock([\n",
    "                            self.down,\n",
    "                            ConvBlock(cs[3], cs[4]),\n",
    "                            ConvBlock(cs[4], cs[4]),\n",
    "                            SkipBlock([\n",
    "                                self.down,\n",
    "                                ConvBlock(cs[4], cs[5]),\n",
    "                                ConvBlock(cs[5], cs[5]),\n",
    "                                ConvBlock(cs[5], cs[5]),\n",
    "                                ConvBlock(cs[5], cs[4]),\n",
    "                                self.up,\n",
    "                            ]),\n",
    "                            ConvBlock(cs[4] * 2, cs[4]),\n",
    "                            ConvBlock(cs[4], cs[3]),\n",
    "                            self.up,\n",
    "                        ]),\n",
    "                        ConvBlock(cs[3] * 2, cs[3]),\n",
    "                        ConvBlock(cs[3], cs[2]),\n",
    "                        self.up,\n",
    "                    ]),\n",
    "                    ConvBlock(cs[2] * 2, cs[2]),\n",
    "                    ConvBlock(cs[2], cs[1]),\n",
    "                    self.up,\n",
    "                ]),\n",
    "                ConvBlock(cs[1] * 2, cs[1]),\n",
    "                ConvBlock(cs[1], cs[0]),\n",
    "                self.up,\n",
    "            ]),\n",
    "            ConvBlock(cs[0] * 2, cs[0]),\n",
    "            nn.Conv2d(cs[0], 3, 3, padding=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, input, t):\n",
    "        timestep_embed = expand_to_planes(self.timestep_embed(t[:, None]), input.shape)\n",
    "        v = self.net(torch.cat([input, timestep_embed], dim=1))\n",
    "        alphas, sigmas = map(partial(append_dims, n=v.ndim), t_to_alpha_sigma(t))\n",
    "        pred = input * alphas - v * sigmas\n",
    "        eps = input * sigmas + v * alphas\n",
    "        return DiffusionOutput(v, pred, eps)\n",
    "\n",
    " \n",
    "secondary_model = SecondaryDiffusionImageNet2()\n",
    "secondary_model.load_state_dict(torch.load('secondary_model_imagenet_2.pth', map_location='cpu'))\n",
    "secondary_model = secondary_model.eval().requires_grad_(False).to(\"cuda\") \n",
    "\n",
    "from functools import partial\n",
    "\n",
    "from guided_diffusion.script_util import create_model_and_diffusion, model_and_diffusion_defaults\n",
    "model_config = model_and_diffusion_defaults()\n",
    "model_config.update({\n",
    "    'attention_resolutions': '32,16,8',\n",
    "    'class_cond': False,\n",
    "    'diffusion_steps': 1000,\n",
    "    'rescale_timesteps': True,\n",
    "    'timestep_respacing':\"16,48,72\", \n",
    "    'image_size': 512,\n",
    "    'learn_sigma': True,\n",
    "    'noise_schedule': 'linear',\n",
    "    'num_channels': 256,\n",
    "    'num_head_channels': 64,\n",
    "    'num_res_blocks': 2,\n",
    "    'resblock_updown': True,\n",
    "    'use_fp16': True,\n",
    "    'use_scale_shift_norm': True,\n",
    "    'use_checkpoint': True\n",
    "})\n",
    "\n",
    "def wrapped_openai(x, t):\n",
    "    x = x\n",
    "    t = t\n",
    "    return openai(x, t * 1000)[:, :3]\n",
    "\n",
    "\n",
    "def cfg_model_fn(x, t):\n",
    "    n = x.shape[0]\n",
    "    n_conds = len(target_embeds[\"ViT-B/16\"])\n",
    "    x_in = x.repeat([n_conds, 1, 1, 1])\n",
    "    t_in = t.repeat([n_conds])\n",
    "    clip_embed_in = target_embeds[\"ViT-B/16\"].repeat_interleave(n, 0)\n",
    "    vs = model[\"cc12m_1_cfg\"](x_in, t_in, clip_embed_in).view([n_conds, n, *x.shape[1:]])\n",
    "    v = vs.mul(weights[\"ViT-B/16\"][:, None, None, None, None]).sum(0)\n",
    "    #display.clear_output(wait=True)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Fpbody2NCR7w"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "from diffusion import get_model, get_models, utils\n",
    "from pytorch_lit import LitModule\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "\n",
    "model_list = [\n",
    "  #\"cc12m_1_cfg\",\n",
    "  \"yfcc_2\",\n",
    "  \"openimages\"\n",
    "             ]\n",
    "\n",
    "use_LIT = False\n",
    "model = {}\n",
    "\n",
    "if use_LIT:\n",
    "    for model_name in model_list:\n",
    "        checkpoint = \"models/v-diffusion/\"+model_name+\".pth\"\n",
    "        if model_name != \"openimages\":\n",
    "            model[model_name] = get_model(model_name)()\n",
    "            model[model_name] = model[model_name].to(device).eval().requires_grad_(False)\n",
    "            model[model_name] = LitModule.from_params(\"models/\"+model_name,\n",
    "                                      lambda: model[model_name],\n",
    "                                      device=\"cuda\")\n",
    "        elif model_name == \"openimages\":\n",
    "            openai, diffusion = create_model_and_diffusion(**model_config)\n",
    "            openai.load_state_dict(torch.load('models/v-diffusion/openimages.pth', map_location='cpu'))\n",
    "            openai.requires_grad_(False).eval().to(device)\n",
    "\n",
    "            for name, param in openai.named_parameters():\n",
    "                if 'qkv' in name or 'norm' in name or 'proj' in name:\n",
    "                    param.requires_grad_()\n",
    "            if model_config['use_fp16']:\n",
    "                openai.convert_to_fp16()\n",
    "            openai = LitModule.from_params(\"models/openimages\",\n",
    "                                      lambda: openai,\n",
    "                                      device=\"cuda\")\n",
    "            model[\"openimages\"] = wrapped_openai\n",
    "else:\n",
    "    for model_name in model_list:\n",
    "        checkpoint = \"models/v-diffusion/\"+model_name+\".pth\"\n",
    "        if model_name != \"openimages\":\n",
    "            model[model_name] = get_model(model_name)()\n",
    "            model[model_name].load_state_dict(torch.load(checkpoint, map_location='cpu'))\n",
    "            model[model_name] = model[model_name].half()\n",
    "            model[model_name] = model[model_name].to(device).eval().requires_grad_(False)\n",
    "        elif model_name == \"openimages\":\n",
    "            openai, diffusion = create_model_and_diffusion(**model_config)\n",
    "            openai.load_state_dict(torch.load('models/v-diffusion/openimages.pth', map_location='cpu'))\n",
    "            openai.requires_grad_(False).eval().to(device)\n",
    "            for name, param in openai.named_parameters():\n",
    "                if 'qkv' in name or 'norm' in name or 'proj' in name:\n",
    "                    param.requires_grad_()\n",
    "            if model_config['use_fp16']:\n",
    "                openai.convert_to_fp16()\n",
    "            model[\"openimages\"] = wrapped_openai\n",
    "            \n",
    "if \"cc12m_1_cfg\" in model_list:\n",
    "    model[\"cc12m_1\"]=cfg_model_fn\n",
    "\n",
    "        \n",
    "normalize = transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
    "                                     std=[0.26862954, 0.26130258, 0.27577711])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "VnQjGugaDZPJ",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ViT-L/14': 224, 'RN50x64': 448}\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: c:\\program files\\python39\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n"
     ]
    }
   ],
   "source": [
    "# Load models\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "clip_list = [\n",
    "\"ViT-L/14\",     #VRAM HEAVY!!!\n",
    "\"RN50x64\",     #VRAM HEAVY!!!\n",
    "# 'RN50x16',    #VRAM HEAVY!!!\n",
    "# 'ViT-B/32',\n",
    "# \"ViT-B/16\",\n",
    "# \"RN50x4\",\n",
    " #\"RN101\",\n",
    " # \"RN50\"\n",
    "]\n",
    "\n",
    "clip_model = {}\n",
    "clip_size = {}\n",
    "for i in clip_list:\n",
    "    clip_model[i] = clip.load(i, jit=False)[0].eval().requires_grad_(False).to(device)\n",
    "    clip_size[i] = clip_model[i].visual.input_resolution\n",
    "    \n",
    "print(clip_size)\n",
    "normalize = transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
    "                                 std=[0.26862954, 0.26130258, 0.27577711])\n",
    "lpips_model = lpips.LPIPS(net='vgg').to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9zY-8I90LkC6"
   },
   "source": [
    "## Settings for this run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "U0PwzFZbLfcy"
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "import threading\n",
    "\n",
    "from tqdm import trange\n",
    "\n",
    "def cond_clamp(image): \n",
    "    #if t >=0:\n",
    "        mag=image.square().mean().sqrt()\n",
    "        mag = (mag*cc).clamp(1.6,100)\n",
    "        image = image.clamp(-mag, mag)\n",
    "        return(image)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def cond_sample(model, x, steps, eta, extra_args, cond_fn):\n",
    "    \"\"\"Draws guided samples from a model given starting noise.\"\"\"\n",
    "    global clamp_max\n",
    "    ts = x.new_ones([x.shape[0]])\n",
    "\n",
    "    # Create the noise schedule\n",
    "    alphas, sigmas = utils.t_to_alpha_sigma(steps)\n",
    "\n",
    "    # The sampling loop\n",
    "    for i in trange(len(steps)):\n",
    "        if pace[i%len(pace)][\"model_name\"]==\"cc12m_1\":\n",
    "            extra_args_in = extra_args\n",
    "        else:\n",
    "            extra_args_in= {}\n",
    "\n",
    "        # Get the model output\n",
    "        with torch.enable_grad():\n",
    "            x = x.detach().requires_grad_()\n",
    "            with torch.cuda.amp.autocast():\n",
    "                if lerp:\n",
    "                    v=torch.zeros_like(x)\n",
    "                    for j in pace:\n",
    "                        if j[\"model_name\"]==\"cc12m_1\":\n",
    "                            extra_args_in = extra_args\n",
    "                        else:\n",
    "                            extra_args_in= {}\n",
    "                        v += model[j[\"model_name\"]](x, ts * steps[i], **extra_args_in)\n",
    "                    v = v/len(pace)\n",
    "                else:\n",
    "                    v = model[pace[i%len(pace)][\"model_name\"]](x, ts * steps[i], **extra_args_in)\n",
    "            v = cond_clamp(v)\n",
    "\n",
    "        if use_secondary_model:\n",
    "            with torch.no_grad():\n",
    "                if steps[i] < 1 and pace[i%len(pace)][\"guided\"]:\n",
    "                    pred = x * alphas[i] - v * sigmas[i]\n",
    "                    cond_grad = cond_fn(x, ts * steps[i],pred, **extra_args).detach()\n",
    "                    v = v.detach() - cond_grad * (sigmas[i] / alphas[i]) * pace[i%len(pace)][\"mag_adjust\"]\n",
    "                else:\n",
    "                    v = v.detach()\n",
    "                    pred = x * alphas[i] - v * sigmas[i]\n",
    "                    clamp_max=torch.tensor([0])\n",
    "\n",
    "        else:\n",
    "            if steps[i] < 1 and pace[i%len(pace)][\"guided\"]:\n",
    "                with torch.enable_grad():\n",
    "                    pred = x * alphas[i] - v * sigmas[i]\n",
    "                    cond_grad = cond_fn(x, ts * steps[i],pred, **extra_args).detach()\n",
    "                    v = v.detach() - cond_grad * (sigmas[i] / alphas[i]) * pace[i%len(pace)][\"mag_adjust\"]\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    v = v.detach()\n",
    "                    pred = x * alphas[i] - v * sigmas[i]\n",
    "                    clamp_max=torch.tensor([0])\n",
    "\n",
    "        mag = pred.square().mean().sqrt()\n",
    "      #  print(mag)\n",
    "        if torch.isnan(mag):\n",
    "            print(\"ERROR2\")\n",
    "            continue\n",
    "        \n",
    "        filename = f'content/diff/{taskname}_N.jpg'\n",
    "        TF.to_pil_image(pred[0].add(1).div(2).clamp(0, 1)).save(filename,quality=99)\n",
    "        textprogress.value = f'{taskname},  step {round(steps[i].item()*1000)}, {pace[i%len(pace)][\"model_name\"]} :'\n",
    "        file = open(filename, \"rb\")\n",
    "        image=file.read()\n",
    "        progress.value = image \n",
    "        file.close()\n",
    "            \n",
    "        # Predict the noise and the denoised image\n",
    "        pred = x * alphas[i] - v * sigmas[i]\n",
    "        eps = x * sigmas[i] + v * alphas[i]\n",
    "\n",
    "        # If we are not on the last timestep, compute the noisy image for the\n",
    "        # next timestep.\n",
    "        if i < len(steps) - 1:\n",
    "            # If eta > 0, adjust the scaling factor for the predicted noise\n",
    "            # downward according to the amount of additional noise to add\n",
    "            if eta >=0:\n",
    "                ddim_sigma = eta * (sigmas[i + 1]**2 / sigmas[i]**2).sqrt() * \\\n",
    "                    (1 - alphas[i]**2 / alphas[i + 1]**2).sqrt()\n",
    "            else:\n",
    "                ddim_sigma = -eta*sigmas[i+1]\n",
    "            adjusted_sigma = (sigmas[i + 1]**2 - ddim_sigma**2).sqrt()\n",
    "\n",
    "            # Recombine the predicted noise and predicted denoised image in the\n",
    "            # correct proportions for the next step\n",
    "            x = pred * alphas[i + 1] + eps * adjusted_sigma\n",
    "            x = cond_clamp(x)\n",
    "\n",
    "\n",
    "            # Add the correct amount of fresh noise\n",
    "            if eta:\n",
    "                x += torch.randn_like(x) * ddim_sigma\n",
    "            \n",
    "         #######   x = sample_a_step(model, x.detach(), steps2, i//2, eta, extra_args)\n",
    "\n",
    "\n",
    "    # If we are on the last timestep, output the denoised image\n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nf9hTc8YLoLx"
   },
   "source": [
    "### Actually do the run..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clamp_start_=0\n",
    "def cond_fn(x, t, x_in, clip_embed=[]):\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        global test, clamp_start_, clamp_max\n",
    "        t2=t\n",
    "        t=round(t.item()*1000)\n",
    "        n = x.shape[0]\n",
    "        with torch.enable_grad():\n",
    "            if use_secondary_model:                 \n",
    "                x = x.detach().requires_grad_()\n",
    "                x_in_second = secondary_model(x, t2.repeat([n])).pred\n",
    "                if use_original_as_clip_in: x_in = replace_grad(x_in, (1-use_original_as_clip_in)*x_in_second+use_original_as_clip_in*x_in)\n",
    "                else : x_in = x_in_second\n",
    "\n",
    "\n",
    "            x_in_grad = torch.zeros_like(x_in)\n",
    "            clip_guidance_scale = clip_guidance_index[1000-t]\n",
    "#             clamp_max = clamp_index[1000-t]\n",
    "            make_cutouts = {}\n",
    "            cutn = cut_innercut[1000-t] + cut_overview[1000-t]\n",
    "            for i in clip_list:\n",
    "                make_cutouts[i] = MakeCutoutsVDango(clip_size[i],\n",
    "                 Overview= cut_overview[1000-t], \n",
    "                 InnerCrop = cut_innercut[1000-t], IC_Size_Pow=cut_ic_pow, IC_Grey_P = cut_icgray_p[1000-t]\n",
    "                 )\n",
    "            nscut = MakeCutoutsVDango(200, Overview=1)\n",
    "            add_cuts = nscut(x_in.add(1).div(2))\n",
    "            for k in range(cutn_batches):\n",
    "                    losses=0\n",
    "                    for i in clip_list:\n",
    "                        clip_in = normalize(make_cutouts[i](x_in.add(1).div(2)).to(\"cuda\"))\n",
    "                        image_embeds = clip_model[i].encode_image(clip_in).float()\n",
    "                        image_embeds = image_embeds.unsqueeze(1)\n",
    "                        dists = spherical_dist_loss(image_embeds, target_embeds[i].unsqueeze(0))\n",
    "                        del image_embeds, clip_in\n",
    "                        dists = dists.view([cutn, n, -1])\n",
    "                        losses = dists.mul(weights[i]).sum(2).mean(0)\n",
    "                        x_in_grad += torch.autograd.grad(losses.sum() * clip_guidance_scale, x_in)[0] / cutn_batches / len(clip_list)          \n",
    "                        del dists,losses\n",
    "                    gc.collect()\n",
    "                    \n",
    "            tv_losses = tv_loss(x_in).sum() * tv_scales[0] +\\\n",
    "                tv_loss(F.interpolate(x_in, scale_factor= 1/2)).sum()* tv_scales[1] + \\\n",
    "                tv_loss(F.interpolate(x_in, scale_factor = 1/4)).sum()* tv_scales[2] + \\\n",
    "                tv_loss(F.interpolate(x_in, scale_factor = 1/8)).sum()* tv_scales[3] \n",
    "            sat_scale = sat_index[1000-t]\n",
    "            range_scale= range_index[1000-t]\n",
    "            range_losses = range_loss(x_in,RGB_min,RGB_max).sum() * range_scale\n",
    "            sat_losses = range_loss(x,-1.0,1.0).sum() * sat_scale + tv_loss(x).sum() * tv_scale_2\n",
    "            try:\n",
    "                bsq_loss = brisque(x_in.add(1).div(2).clamp(0,1),data_range=1.)\n",
    "            except:\n",
    "                bsq_loss=0\n",
    "            if bsq_loss <=10 : bsq_loss = 0\n",
    "            \n",
    "            loss =  tv_losses  + range_losses  + \\\n",
    "                bsq_loss * bsq_scale \n",
    "\n",
    "            if init is not None and init_scale:\n",
    "                init_losses = lpips_model(x_in, init)\n",
    "                loss = loss + init_losses.sum() * init_scale\n",
    "            loss_grad = torch.autograd.grad(loss, x_in, )[0]\n",
    "            sat_grad = torch.autograd.grad(sat_losses, x, )[0]\n",
    "            x_in_grad += loss_grad + sat_grad\n",
    "            x_in_grad = torch.nan_to_num(x_in_grad, nan=0.0, posinf=0, neginf=0)\n",
    "            grad = -torch.autograd.grad(x_in, x, x_in_grad)[0]\n",
    "            grad = torch.nan_to_num(grad, nan=0.0, posinf=0, neginf=0)\n",
    "            mag = grad.square().mean().sqrt()\n",
    "            if mag==0:\n",
    "                print(\"ERROR\")\n",
    "                return(grad)\n",
    "            if t>=0:\n",
    "                if active_function == \"softsign\":\n",
    "                    grad = F.softsign(grad*grad_scale/mag)\n",
    "                if active_function == \"tanh\":\n",
    "                    grad = (grad/mag*grad_scale).tanh()\n",
    "                if active_function==\"clamp\":\n",
    "                    grad = grad.clamp(-mag*grad_scale*2,mag*grad_scale*2)\n",
    "            if grad.abs().max()>0:\n",
    "                grad=grad/grad.abs().max()\n",
    "                magnitude = grad.square().mean().sqrt()\n",
    "            else:\n",
    "                print(grad)\n",
    "                return(grad)\n",
    "            clamp_max = clamp_index[1000-t]\n",
    "        return grad* magnitude.clamp(max= clamp_max) /magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "X5gODNAMEUCR",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ViT-L/14': tensor([0., 2.], device='cuda:0'), 'RN50x64': [1.0]}\n",
      "{'ViT-L/14': tensor([0., 2.], device='cuda:0'), 'RN50x64': tensor([0., 2.], device='cuda:0')}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f066daa968444205970ef8bd8e6b9b60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24525ca21d634577859a9cfd5cd36b6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image(value=b'', layout=\"Layout(max_height='512px', max_width='400px')\")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/150 [00:00<?, ?it/s]c:\\program files\\python39\\lib\\site-packages\\torch\\nn\\functional.py:3657: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\n",
      "  7%|█████▉                                                                           | 11/150 [00:15<03:08,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|█████████████████████                                                            | 39/150 [00:53<02:30,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|███████████████████████████████████████████████████████████████████████████████▍| 149/150 [03:23<00:01,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR\n",
      "ERROR2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 150/150 [03:24<00:00,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR\n",
      "ERROR2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "def do_run():\n",
    "    global target_embeds, weights, init, makecutouts, progress, textprogress, progress2, batch_num,taskname\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "    make_cutouts = {}\n",
    "    for i in clip_list:\n",
    "         make_cutouts[i] = MakeCutoutsVDango(clip_size[i],Overview=1)\n",
    "    side_x, side_y = [w,h]\n",
    "    target_embeds, weights ,zero_embed = {}, {}, {}\n",
    "    for i in clip_list:\n",
    "        zero_embed[i] = torch.zeros([1, clip_model[i].visual.output_dim], device=device)\n",
    "        target_embeds[i] = [zero_embed[i]]\n",
    "        weights[i]=[]\n",
    "\n",
    "    for prompt in prompts:\n",
    "        txt, weight = parse_prompt(prompt)\n",
    "        for i in clip_list:\n",
    "            embeds = clip_model[i].encode_text(clip.tokenize(txt).to(device)).float()\n",
    "            target_embeds[i].append(embeds)\n",
    "            weights[i].append(weight)\n",
    "\n",
    "    for prompt in image_prompts:\n",
    "        print(f\"processing{prompt}\",end=\"\\r\")\n",
    "        path, weight = parse_prompt(prompt)\n",
    "        img = Image.open(fetch(path)).convert('RGB')\n",
    "        img = TF.resize(img, min(side_x, side_y, *img.size), transforms.InterpolationMode.LANCZOS)\n",
    "        for i in clip_list:\n",
    "            batch = make_cutouts[i](TF.to_tensor(img).unsqueeze(0).to(device))\n",
    "            embed = clip_model[i].encode_image(normalize(batch)).float()\n",
    "            target_embeds[i].append(embed)\n",
    "            weights[i].extend([weight])\n",
    "        \n",
    "    if anti_jpg!=0:\n",
    "        target_embeds[\"ViT-B/32\"].append(torch.tensor([np.load(\"openimages_512x_png_embed224.npz\")['arr_0']-np.load(\"imagenet_512x_jpg_embed224.npz\")['arr_0']], device = device))\n",
    "        weights[i].append(anti_jpg)\n",
    "\n",
    "    for i in clip_list:\n",
    "        target_embeds[i] = torch.cat(target_embeds[i])\n",
    "        weights[i] = torch.tensor([1 - sum(weights[i]), *weights[i]], device=device)\n",
    "        weights[i] = weights[i]/weights[i].abs().sum() * 2\n",
    "        print(weights)\n",
    "        \n",
    "    init = None\n",
    "    init_mask = None\n",
    "    if init_image is not None:\n",
    "        S = model_config['image_size']\n",
    "        if mask_scale > 0:\n",
    "            init = Image.open(fetch(init_image)).convert('RGBA')\n",
    "            init = init.resize((S, S), Image.BILINEAR)\n",
    "            init = TF.to_tensor(init).to(device)\n",
    "            init_mask = init[3] # alpha channel\n",
    "            init_mask = (init_mask>0.5).to(torch.float32)\n",
    "            init = init[:3].unsqueeze(0).mul(2).sub(1) # RGB\n",
    "        else:\n",
    "            init = Image.open(fetch(init_image)).convert('RGB')\n",
    "            init = init.resize((S, S), Image.LANCZOS)\n",
    "            init = TF.to_tensor(init).to(device)\n",
    "            init = init.unsqueeze(0).mul(2).sub(1)\n",
    "\n",
    "    cur_t = None\n",
    "    \n",
    "    for i in range(n_batches):\n",
    "        taskname=taskname_+\"_\"+str(i)\n",
    "                # Display Handling\n",
    "        from IPython.display import display\n",
    "        import ipywidgets as widgets\n",
    "        import threading\n",
    "\n",
    "        t = torch.linspace(1, 0, step + 1, device=device)[:-1]\n",
    "        t=t.pow(steps_pow)\n",
    "        x = torch.randn([1, 3, side_y, side_x], device=device)\n",
    "        steps = utils.get_spliced_ddpm_cosine_schedule(t)\n",
    "        if \"cc12m_1\" in model_list:\n",
    "            extra_args = {'clip_embed': target_embeds[\"ViT-B/16\"][0].unsqueeze(0)}\n",
    "        else:\n",
    "            extra_args = {}\n",
    "        progress = widgets.Image(layout = widgets.Layout(max_width = \"400px\",max_height = \"512px\"))\n",
    "        textprogress = widgets.Textarea()\n",
    "        display(textprogress)\n",
    "        display(progress)\n",
    "        cond_sample(model, x, steps, eta, extra_args, cond_fn)\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "batch_num=0\n",
    "clamp_start_=0\n",
    "seed = 42\n",
    "title = \"AA\"\n",
    "init_scale = 0 # This enhances the effect of the init image, a good value is 1000.\n",
    "init_image = None# \"content/init/96.jpg\"   # This can be an URL or Colab local path and must be in quotes.\n",
    "mask_scale=0\n",
    "init_mask = None\n",
    "\n",
    "RGB_min, RGB_max = [-0.95,0.95]\n",
    "\n",
    "\n",
    "\n",
    "n_batches = 1\n",
    "cutn_batches = 1\n",
    "\n",
    "cut_overview = [10]*200+[10]*1000\n",
    "cut_innercut = [0]*200+[0]* 1000\n",
    "cut_ic_pow = 0.3\n",
    "cut_icgray_p = [0]*100+[0]*100+[0]*100+[0]*1000\n",
    "aug=True\n",
    "\n",
    "padargs = {\"mode\":\"constant\", \"value\":-1}\n",
    "flip_aug=False\n",
    "cutout_debug = False\n",
    "\n",
    "clip_guidance_index = [240000]*1000\n",
    "anti_jpg=0\n",
    "w,h = 32*12,32*16\n",
    "\n",
    "\n",
    "tv_scales = [0]+[2000]*3\n",
    "tv_scale_2 = 0\n",
    "\n",
    "step = 150\n",
    "steps_pow=1\n",
    "\n",
    "pace=[\n",
    "#{\"model_name\":\"cc12m_1\", \"guided\":True, \"mag_adjust\":1},\n",
    "{\"model_name\":\"openimages\", \"guided\":True, \"mag_adjust\":1},\n",
    "{\"model_name\":\"yfcc_2\", \"guided\":True, \"mag_adjust\":1},\n",
    "]\n",
    "\n",
    "\n",
    "clamp_index = 1*np.array([0.03]*50+[0.04]*100+[0.05]*850)\n",
    "sat_index =   np.array([0]*40+[0]*960)\n",
    "range_index= np.array([0]*50 +[0]*950) \n",
    "cfg_scale=4\n",
    "eta=1.4\n",
    "\n",
    "image_prompts = []\n",
    "use_secondary_model=False\n",
    "use_original_as_clip_in=0\n",
    "lerp=True # Vram heavy!!\n",
    "\n",
    "\n",
    "for prompts in  [\n",
    "[\n",
    "\"Portrait of Princess victoria, trending on artstation\"\n",
    "]\n",
    "]: \n",
    "    for cc in [6]:\n",
    "        for bsq_scale in [0]:\n",
    "              for grad_scale in [2]:\n",
    "                for active_function in [\"softsign\"]:\n",
    "                    torch.manual_seed(seed)\n",
    "                    random.seed(seed)\n",
    "                    if grad_scale!=1 and active_function==\"NA\": continue\n",
    "                    title2 = title + str(int(time.time()))\n",
    "                    taskname_ = title2 +  \"_eta\"  + str(eta)+\"_cc\"+str(cc)+\"_gs\"+str(grad_scale)#+ prompts[0]\n",
    "                    do_run()\n",
    "                    gc.collect()\n",
    "\n",
    "\n",
    "                    \n",
    "#gc.collect()\n",
    "#do_run() n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CLIP Guided Diffusion HQ 512x512 Uncond.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
